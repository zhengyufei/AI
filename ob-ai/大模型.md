
conda create -n python311 python=3.11
source activate python311
conda activate python311

curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash

sudo apt-get install git-lfs

GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm2-6b

git lfs pull --include="*.bin"

git lfs pull --include="*.model"

python ./tokenize_dataset_rows.py --jsonl_path ./data/wenlv_data.jsonl --save_path ./data/wenlv_token --max_seq_length 300

nvidia-smi

RuntimeError: CUDA out of memory. Tried to allocate 2.32 GiB (GPU 0; 22.20 GiB total capacity; 18.80 GiB already allocated; 950.12 MiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

基座模型，对话模型

不同大模型用法不一样，在Hugging Face可以查看描述，比如glm-10b擅长完形填空

常见数据集 
1 English CommonCrawl 
使用模型：LLaMA（67%）、LaMDA、PaLM 
下载地址 https://github.com/karust/gogetcrawl 
2 Wikipedia 
使用模型：LLaMA（4.5%）、GPT-NEOX（1.53%）、LaMDA、PaLM 
下载地址 https://huggingface.co/datasets/wikipedia 
3 C4 
使用模型：LLaMA（15%）、LaMDA、PaLM 
下载地址 https://huggingface.co/datasets/c4 
4 Github 
使用模型：LLaMA（4.5%）、GPT-NEOX（7.59%）、PaLM、OPT、GLM130B 
下载地址 https://github.com/EleutherAI/github-downloader 
美洲驼LLAMA数据 
模型与数据参数量：30B/65B 1.4T 300B tokens 
数据下载地址 https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T 
BLOOM 
模型与数据参数量：1.6TB 350B tokens 
数据下载地址 https://docs.google.com/forms/d/e/1FAIpQLSdq50O1x4dkdGI4dwsmchFuNI0KCWEDiKUYxvd0r0_sl6FfAQ/viewform?pli=1 
中文数据集 
1 中文文本分类数据集THUCNews http://thuctc.thunlp.org 
2 清华大学NLP实验室开放数据集 http://thuocl.thunlp.org/ 
3 wiki百科中文 https://zh.wikipedia.org 
4 WuDaoCorpora https://openi.pcl.ac.cn/BAAI/WuDao-Data/ 
5 Chinese book https://link.zhihu.com/?target=https%3A//github.com/JiangYanting/Chinese_book_dataset 
6 千言 https://www.luge.ai/

幂率 Scaling Laws
决定效果:  模型参数量 数据集大小 计算量
性价比是一个问题
三者比例也要合适，不能单独增加一个
数据量如果不是很大，就不要选特别大的模型 

分词粒度： 
1.单词分词法：英文（空格分词），中文（jieba分词 or 分字） 
2.单字分词法：英文（字母），中文（分字） 
3.子词分词法：BPE，WordPiece，Unigram

为什么要生成子词，只微调需要生成子词吗

Maxlength：一般会统计平均长度（去除异常值） 平均长度*1.5 确定好训练的参数 
Batchsize理论上来说，越大越好